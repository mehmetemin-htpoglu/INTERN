import chainlit as cl
import ollama

@cl.on_chat_start
async def start():
    # Test iÃ§in gerekli ayarlarÄ± yÃ¼klÃ¼yoruz
    cl.user_session.set("model", "deepseek-r1:14b")
    cl.user_session.set("temperature", 0.7)
    # BoÅŸ bir geÃ§miÅŸle baÅŸlayalÄ±m
    cl.user_session.set("message_history", [])
    
    await cl.Message(content="ğŸ•µï¸ **Dedektif Modu Aktif!**\nLÃ¼tfen bir soru sor ve VS Code terminalini izle.").send()

@cl.on_message
async def main(message: cl.Message):
    user_input = message.content
    model = cl.user_session.get("model")
    current_temp = cl.user_session.get("temperature")
    message_history = cl.user_session.get("message_history")
    
    # MesajÄ± geÃ§miÅŸe ekle
    message_history.append({"role": "user", "content": user_input})

    msg = cl.Message(content="")
    
    # Terminalde baÅŸlangÄ±cÄ± iÅŸaretleyelim
    print(f"\n{'='*20} STREAM BAÅLIYOR (HAM VERÄ°) {'='*20}") 

    stream = ollama.chat(
        model=model,
        messages=message_history,
        options={'temperature': current_temp},
        stream=True
    )

    full_response = ""

    for chunk in stream:
        token = chunk['message']['content']
        
        # 1. HAM VERÄ°YÄ° TERMINALE BAS (Filtresiz)
        # flush=True ile anÄ±nda yazmasÄ±nÄ± saÄŸlÄ±yoruz, bekleme yapmaz
        print(token, end="", flush=True) 
        
        full_response += token
        
        # 2. ARAYÃœZE BAS (Chainlit ne yapÄ±yor gÃ¶relim)
        await msg.stream_token(token)
    
    print(f"\n{'='*20} STREAM BÄ°TTÄ° {'='*20}\n")
    
    message_history.append({"role": "assistant", "content": full_response})
    cl.user_session.set("message_history", message_history)
    
    await msg.send()